SDNN : Selective Desensitization Neural Network
===============================================

多層パーセプトロン(MLP)は「任意の可積分な連続関数を任意の精度で近似するMLPが存在する」という万能の定理があり、この定理を元にMLPはさまざまな分野でつかわれている。しかしMLPは複数の分散表現を統合することが困難であり、期待される汎化が生じないことがある。
SDNNはMLPに選択的不感化という手法を導入してMLPの問題点を解決し、
高い学習能力と汎化能力に加えて計算コストやパラメータ調整の容易さなど、実用的な面で優れた性質を持つ
選択的不感化に加えてMLPと異なる点は実数を区間で区切ってそれぞれの区間を0と1で表現されるベクトルで表した点。
入力を実数ではなくバイナリベクトルとして、中間素子から出力素子の結合係数を固定したニューラルネットを並列パーセプトロンという。
並列パーセプトロンの中間層の活性化関数はシグモイドではなくヘビサイド関数を使う。
学習は誤差に応じて修正する素子の数を決めて、教師信号よりもニューラルネットの出力値が大きい場合は、閾値より大きい素子のうち閾値に近い素子を対象とする。
逆に教師信号よりも出力値が小さい場合は、閾値より小さい素子のうち閾値に近い素子を対象とする。
素子の修正方法は最急降下法を用いる。
選択的不感化ニューラルネットは並列パーセプトロンに選択的不感化という処理を加えたものである。

疑問点
======
* パターンコーディングによってパラメータ数が爆発的に増加するが、何故オーバーフィティングが起きないのか
    - パラメータ数に対してサンプル数が大きいとオーバーフィティングは起きない
    - 選択的不感化・学習規則が正則化の役割を果たしている
* 教師データにノイズが乗っている場合においても汎化性能は変わらないか

改善点
======
* 正則化項の検討
* 共役勾配法や準ニュートン法のように再急降下法よりも高性能な学習規則を使う
* 関連度自動決定?

